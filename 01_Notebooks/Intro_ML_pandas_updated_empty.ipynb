{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning in Python\n",
    "The pupose of this notebook is to introduce you to some basic concepts and techniques for machine learning in Python. \n",
    "\n",
    "**This notebook will cover:**\n",
    "1. Setting up JupyterNotebook and importing libraries\n",
    "2. The dataset used in this notebook\n",
    "3. Reading files into a pandas DataFrame\n",
    "3. Looking closer at data in the read file using pands built-in functionality and python basics\n",
    "5. Vizualizing data using matplotlib and seaborn\n",
    "6. Wrangling data\n",
    "7. Bonus: Intro to Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites for using Jupyter Notebook\n",
    "I recommend installing [Anaconda](https://www.anaconda.com/download). This enables you to run a JupyterNotebook on your local computer in the browser and manage Python libraries. There are alternative solutions but this get you up and running very quickly.\n",
    "\n",
    "* Once installed, navigate to `Home` and click on the `Launch` button for the `JupyterNotebook` application. This will open a new tab in your browser.\n",
    "* Navigate to this file in your folder stucture and double click it, this should open a new tab in your browser.\n",
    "* Now you are good to go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries, and reading data\n",
    "Before diving into the details we start by importing the necessary Python libraries for this training. Click the respective library names for further information.\n",
    "\n",
    "> | Library | Description |\n",
    "> | --- | --- |\n",
    "> | [random](https://docs.python.org/3/library/random.html) | for random number generation |\n",
    "> | [pandas](https://pandas.pydata.org/) | for data manipulation and analysis (dataframes) |\n",
    "> | [numpy](https://numpy.org/) | for numerical operations |\n",
    "> | [matplotlib](https://matplotlib.org/) | visualization with Python |\n",
    "> | [scikit-learn](https://scikit-learn.org/stable/) | machine learning in Python |\n",
    "> | [seaborn](https://seaborn.pydata.org/) | statistical data visualization |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code installs all necessary python libraries from the requirements.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each library is imported as \"short_name\" to enables shorter reference names later, e.g., random is imported as rd\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns # Add to importing libraries table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset used in this notebook\n",
    "\n",
    "In this notebook we will utilize the [Titanic - Machine Learning from Disaster dataset](https://www.kaggle.com/c/titanic/data) available on kaggle. These are available in the *02_Dataset* folder. The dataset is split into two groups:\n",
    "1. The training dataset, train.csv, which should be used to build your ML models.\n",
    "2. The test dataset, test.csv, whihc should be used to evaluate your ML models.\n",
    "\n",
    "**Both datasets contains information about passengers on the Titanic**, in addition to passenger traits such as name, the training dataset contains information about if the passenger survived or not. For an in depth walkthrough of the dataset please visit the provided link above. In short the datasets contins the following attributes for each passanger:\n",
    "\n",
    "> | Variable | Definition | Key |\n",
    "> | --- | --- | --- |\n",
    "> | survival | Survival\t| 0 = No, 1 = Yes |\n",
    "> | pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "> | sex\t| Sex | |\n",
    "> | Age\t| Age in years | |\t\n",
    "> | sibsp |\t# of siblings / spouses aboard the Titanic | |\n",
    "> | parch | # of parents / children aboard the Titanic | |\n",
    "> | ticket | Ticket number | |\t\n",
    "> | fare | Passenger fare | |\t\n",
    "> | cabin | Cabin number | |\t\n",
    "> | embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading csv-files into pandas dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read our two csv-files into a pandas dataframe. This is the first step in getting to know the data. \n",
    "\n",
    "Create two dataframe objects named *df_test* and *df_train* by calling the *read_csv* method of the pandas library. Use the PassengerId as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filepath = \"../02_Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(f\"{dataset_filepath}test.csv\",index_col=0)\n",
    "df_train = pd.read_csv(f\"{dataset_filepath}train.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure it looks good\n",
    "print(f'{len(df_test.columns)} columns and {len(df_train)} rows in the test dataset.')\n",
    "print(f'{len(df_train.columns)} columns and {len(df_test)} rows in the train dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating the data\n",
    "Investigating the data, understanding it and thinking about what we could do with it is the first step in the feature selection process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing is very easy in python using the function *print*. Below we are creating a string *str*. Add code to print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"Machine learning rocks!\"\n",
    "print(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you don't need to always use print when using a jupyter notebook or some sort of IDE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python offers f-strings which allow us to combine a regular string with expressions and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside {} we can but expressions and variables, an typing f'' or f\"\" results in an f-string.\n",
    "f_str = f'{len(df_test.columns)} columns and {len(df_train)} rows in the test dataset.'\n",
    "print(f_str)\n",
    "print(f'{len(df_train.columns)} columns and {len(df_test)} rows in the train dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the data in our dataframes\n",
    "We will now try to select data from the dataframes in different ways in order to understand it better. Dataframes in pandas comes with various methods, such as the `map()` function which enable us to apply a function to every element in a df. Refer to the [pandas documentation](https://pandas.pydata.org/) for a full list of the available methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train # Maybe a bit too long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **Problem 1: Using the head and tail pandas methods**\n",
    ">\n",
    "> Use the `pd.df.head()` and `pd.df.tail()` methods to print some data without getting a massive list. Alter the expression to print 3 or 10 elements of the list. If you want to know more about a pandas methods simply run `?pd.DataFrame.method_name`, e.g., \"?pd.DataFrame.head\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select only the first 3\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select last 10\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **Problem 2: Using the info and description pandas methods**\n",
    ">\n",
    "> Use the `pd.df.info()` and `pd.df.describe()` methods to further understand the data. Which columns does the `describe()` method show?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the dataset types\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which categories do we get?\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add arguments to .describe() to show ordinal data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.DataFrame.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe ordinal data\n",
    "df_train.describe(include='all') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting parts of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very easy to select only parts of the dataframe in pandas.\n",
    "\n",
    "`df_train[['Pclass', 'Survived']]` will return a new dataframe object with only two columns. We can use all the methods we have learned on this new dataframe as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with the columns 'Pclass' and 'Survived'\n",
    "df_train[['Pclass', 'Survived']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **Problem 3: Using the head method to selected parts of the data**\n",
    ">\n",
    "> Use the `head()` method to only show the first 10 entries in the new df above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **Problem 4: Show some basic statistics of the selected data**\n",
    ">\n",
    "> Show some basic statistics of the first ten lines in the df!\n",
    ">\n",
    "> *Hint* use the `describe()`method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **Problem 4: Show some basic statistics of the selected data**\n",
    ">\n",
    "> Append the code above to select only the `Pclass` column from the dataframe returned above. ie., copy your code from the previous problem and add code to only display statistical data for the `Plcass`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **Problem 5: Print the std for Plcass**\n",
    ">\n",
    "> Print the std in the df above, i.e. the standard deviation for the first 10 entries in `Plcass`!\n",
    ">\n",
    "> *Hint:* you select rows with `loc[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.DataFrame.loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is even possible to select columns in the dataframe as attributes and get a pandas series as the return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing the series and dataframes can be done by addin *[a:b]* after the selected df/series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From the first element: 'df_train.Pclass[:4]'\\n\")\n",
    "print(df_train.Pclass[:4])\n",
    "print(\"_\"*60+\"\\n\") # Prints 60 underscores and adds a line break\n",
    "print(\"With start and finish: 'df_train.Pclass[5:8]'\\n\")\n",
    "print(df_train.Pclass[5:8])\n",
    "print(\"_\"*60+\"\\n\")\n",
    "print(\"To the last element: 'df_train.Pclass[887:]'\\n\")\n",
    "print(df_train.Pclass[887:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The last 3 elements 'df_train.Pclass[-3:]'\\n\")\n",
    "print(df_train.Pclass[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in excel, we can pivot data to find interesting cuts and correlations. Notice below how each step of the expression returns a new df which we in turn can apply all df methods and attributes on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=True).mean().sort_values(by='Survived', ascending=False)\n",
    "# Df with selected columns           df grouped by Pclass showing means          df sorted by survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **Problem 6: Examine correlations**\n",
    "> \n",
    "> 1. Examine the correlation between categories `Sex` and `Survived`\n",
    "> \n",
    "> 2. Examine the correlation between `SibSp` and `Survived`\n",
    "> \n",
    "> 3. Examine the correlation between `Parch` and `Survived`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualizing the Data\n",
    "\n",
    "Vizualizing data could be its own training, therefore this section only introduce the basics of [matplotlib](https://matplotlib.org/) and [seaborn](https://seaborn.pydata.org/) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can use plot functions directly on the data structures from pandas\n",
    "In the same way as Pandas has defined the print() to enable easy understanding of the data at hand, pandas have its own plotting functions and methods all relying on [matplotlib](https://matplotlib.org/). We will not cover plotting in depth here, only show how visualization is a natural part of the workflow in understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histrogram of SibSp\n",
    "%matplotlib inline\n",
    "plt.hist(df_train['SibSp'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plotting with seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Difference between matplotlib and seborn:** \"In summary, both Seaborn and Matplotlib have their strengths and weaknesses depending on your specific needs. Seaborn is great for quickly creating visually appealing plots with minimal code, while Matplotlib offers more customization options and fine-grained control over every aspect of a plot.\" - [New Horizons](https://www.newhorizons.com/resources/blog/how-to-choose-between-seaborn-vs-matplotlib#:~:text=In%20summary%2C%20both%20Seaborn%20and,every%20aspect%20of%20a%20plot.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df_train, col='Survived', sharey=True) # Maps \"Survived\" into multiple axes, in this case Survived can be 0 or 1, because of this we get two axes\n",
    "g.map(plt.hist, 'Age', bins=20) # One or more plot functions can be applied to each subset by calling .map(), in this case we plot histograms of \"Age\", one for each lable in \"Survived\", bins specify number of facets we divide the ages into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df_train, col='Survived', row='Pclass', height=2.2, aspect=1.6) # One axes for each possible Survived/Pclass combination\n",
    "g.map(plt.hist, 'Age', alpha=1, bins=20)\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df_train, row='Embarked', height=2.2, aspect=1.6)\n",
    "g.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep', order = [1,2,3], hue_order = ['male','female'])\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df_train, col='Survived',\n",
    "                  row='Embarked', height=2.2, aspect=1.6)\n",
    "g.map(sns.barplot, 'Sex', 'Fare', errorbar=None, order = ['male','female'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **Problem 7: What conclusions can be drawn from our analysis?**\n",
    "> \n",
    "> What conclusions can be drawn from our analysis so far? Look through everything you have done and think about which features seem to be of importance, and how they affect survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling data\n",
    "In this section we will wrangle and manipulate the data to enable analysis. We will learn to drop and change features to enable efficient and accurate modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to drop features we do not need or want. We will have to drop them from both dataframes (train and test). This can be done using the `drop()` function of a pandas dataframe. To make sure we do not make any mistakes, lets compare the shape before or after:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure we apply the same principle to both datasets, but both dataframe objects in a list called `combine` with the \" [ ] \" operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = [df_test, df_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shapes\n",
    "for df in combine:\n",
    "    print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **Problem 8: Drop features**\n",
    "> \n",
    "> We will drop the features `Ticket` and `Cabin`. We pass a list of columns to drop to the function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.DataFrame.drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in combine:\n",
    "    df.drop(....)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we check the shape we will see that nothing has happened to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in combine:\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing has happend to the shape since we created two new dataframes but never stored them anywhere. The `drop()` function returns a new dataframe. What we need to do in order to change the dataframe is to assign it to a variable or use the argument *inplace=True*. \n",
    "\n",
    "* However, setting inplace to True dropped the columns in the original df_train when I ran the code and therefore I would be careful using this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine[0] = combine[0].drop(....) \n",
    "# Here we are calling drop on the first element in the list combine, which contains the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code are calling drop on the first element in the list combine, which contains the train data\n",
    "# However, setting inplace to True dropped the columns in the original df_train when I ran the code and therefore I would be careful using this one\n",
    "# combine[1].drop(....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine[1] = combine[1].drop(....) \n",
    "# Here we are calling drop on the second element in the list combine, which contains the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in combine:\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are pretty sure the name feature contains some interesting data but it is hard to process as is. Lets try to extract the titles from it. Here we are using a regular expression, a subject which is worth a session of its own. In short, the regex here selects all text in the second word followed by a dot. \n",
    "\n",
    "The expand flag makes the function return a new dataframe. This is stored in our dataframe with the column header 'Name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in combine:\n",
    "    df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine[0].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, check if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine[1].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **Problem 9: Create crosstabs**\n",
    ">\n",
    "> Use a `crosstab` to get an understanding for the different titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab from the df_test data using Title and Sex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a crosstab from the df_train data using Title and Sex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace the less frequent titles with 'Rare'. For both datasets, replace the below titles with the title in the comment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with \"Rare\"\n",
    "less_frequent_titles =['Lady', 'Countess','Capt', 'Col',\n",
    "                       'Don', 'Dr', 'Major', 'Rev', 'Sir', \n",
    "                       'Jonkheer', 'Dona']\n",
    "\n",
    "# Replace with Miss\n",
    "miss_replace = ['Mlle', 'Ms']\n",
    "\n",
    "# Replace with Mrs\n",
    "mrs_replace = 'Mme'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.DataFrame.replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **Problem 10: Perfom the replacements**\n",
    ">\n",
    "> Complete the codeblock below to perform the replacements above and get the survival mean for the different titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in combine:\n",
    "    df['Title'] = df['Title'].replace(....)\n",
    "    df['Title'] = df['Title'].replace(....)\n",
    "    df['Title'] = df['Title'].replace(....)\n",
    "    \n",
    "combine[1][['Title', 'Survived']].groupby(['Title'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting feature type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many models have a problem with categorical data. We can convert it into ordinal. However, be careful that the model you are using is not considering it numerical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not careful with which model you are using, many models would consider the title \"Mrs\" as being three times as much title as \"Mr\" which is of course nonsense. This is still categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in combine:\n",
    "    df.drop(['Name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **Problem 11: Convert the sex feature into a categorical feature**\n",
    ">\n",
    "> Let's convert the sex feature into a new categorical feature\n",
    "> \n",
    "> *Hint:* use the `map()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gender_map = {'female':1,'male':0}\n",
    "for df in combine:\n",
    "    df['Sex']= df['Sex']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we examine NaN values (missing values) for each feature using the code block below, we note that a lot of values are missing for age. Consequentially we can either:\n",
    "\n",
    "* Drop all examples in the data with NaN values for any feature\n",
    "* Make an estimated guess to complete the \"Age\" feature in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, df in enumerate(combine):\n",
    "    for feature in df.columns:\n",
    "        number_of_rows_with_na = len(df[df[feature].isna()])\n",
    "        if number_of_rows_with_na != 0:\n",
    "            print(f'{number_of_rows_with_na} NaN rows out of {len(df)} for feature \"{feature}\" in {\"test\" if index == 1 else \"train\"}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how Age correlates with Sex and Pclass to make an educated guess on the missing Age-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(combine[0], row='Pclass', col='Sex', height=2.2, aspect=1.6)\n",
    "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make one guess for each combination of Pclass and Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_ages = np.zeros((2,3))\n",
    "guess_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the test and train dataframes\n",
    "for df in combine:\n",
    "    # Loop over all possible Sex and Pclass combination\n",
    "    for i in range(0, 2): # Sex\n",
    "        for j in range(0, 3): # Pclass\n",
    "            # Save all ages for the given Sex/Pclass combination (drop N/A)\n",
    "            guess_df = df[(df['Sex'] == i) & (df['Pclass'] == j+1)]['Age'].dropna()\n",
    "\n",
    "            # Our guess is that the age of a all samples with a missing age for a given Sex/Pclass combination is the median age \n",
    "            guess_ages[i,j] = guess_df.median() \n",
    "\n",
    "    # Loop over all possible Sex and Pclass combinations        \n",
    "    for i in range(0, 2):\n",
    "        for j in range(0, 3):\n",
    "            # \n",
    "            df.loc[ (df.Age.isnull()) & (df.Sex == i) & (df.Pclass == j+1),'Age'] = guess_ages[i,j]\n",
    "\n",
    "    df['Age'] = df['Age'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if NaN rows remain for \"Age\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, df in enumerate(combine):\n",
    "    for feature in df.columns:\n",
    "        number_of_rows_with_na = len(df[df[feature].isna()])\n",
    "        if number_of_rows_with_na != 0:\n",
    "            print(f'{number_of_rows_with_na} NaN rows out of {len(df)} for feature \"{feature}\" in {\"test\" if index == 1 else \"train\"}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since is only a few rows left in train and test with one missing value we drop these rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine[0] = combine[0].dropna().copy()\n",
    "combine[1] = combine[1].dropna().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Support Vector Machines in Scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure all features are numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine[1].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "embarked_map = {'C':0,'Q':1, 'S':2}\n",
    "for df in combine:\n",
    "    df['Embarked']= df['Embarked'].map(embarked_map).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine[1].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the df into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unscaled = combine[1].loc[:, combine[1].columns != 'Survived']\n",
    "y_train = combine[1]['Survived']\n",
    "X_test_unscaled = combine[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train_unscaled)\n",
    "X_test = sc.transform(X_test_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict test data survival\n",
    "\n",
    "Unfortunately the dataset does not contain any informaiton about the true survival labels in the test data so we cannot validate how \"good\" our SVM is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use a linear kernel, we can examine the coefficients to understand which features have the most influence on the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unscaled.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = abs(clf.coef_[0])\n",
    "plt.bar(X_train_unscaled.columns.to_list(), feature_importance)\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.title(\"Feature Importance for survival prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix for training data\n",
    "\n",
    "Ideally you want to look at the confusion matrix for the test data as well but since the dataset is lacking the true survival labels we can only examine the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{sum(clf.predict(X_train) == y_train) / len(y_train) * 100:.2f}% of the training examples are predicted correct using our SVM.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we examine the confusion matrix for the training data we note that the dataset is very unbalanced. \n",
    "\n",
    "* Only a small number of true labels are \"Survival=1\". \n",
    "\n",
    "* Meaning that if our model where to predict \"Survival=0\" in all cases the precision would be 84.38% (470/557), suggesting that this very simple SVM is note very good at predciting survival. \n",
    "\n",
    "* As a measure to cope with this we could specify class weights but I did that very quickly and did not get better results.\n",
    "\n",
    "* The adjustments we can make are more or loss endless and we can choose a different model to predict survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, y_pred)\n",
    "plt.title(\"Confusion Matrix for Survival Prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "sns.scatterplot(x=X_reduced[:, 0], y=X_reduced[:, 1], hue=y_train)\n",
    "plt.title(\"PCA Projection of Survival Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance\n",
    "print(\"Explained Variance of each component:\", pca.explained_variance_, '\\n')\n",
    "\n",
    "# Explained variance ratio\n",
    "print(\"Explained Variance Ratio of each component:\", pca.explained_variance_ratio_, '\\n')\n",
    "\n",
    "# PCA Components (Eigenvectors)\n",
    "print(\"PCA Components (Eigenvectors):\")\n",
    "print(pca.components_, '\\n')\n",
    "\n",
    "# Singular values\n",
    "print(\"Singular values of each component:\", pca.singular_values_, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
